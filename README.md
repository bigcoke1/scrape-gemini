I worked on this project during my three-month internship at Kaamel Technology. It is developed to make LLMs more flexible to our needs. When the user asks a question, in the default mode, the retrieval model searches for contextual information in the Wikipedia 2017 database, then it provides the contextual information to the GPT model, which will output a refined answer and display it to the user in different formats (sometimes only text and sometimes texts and graphs). Besides the Wikipedia database, users can input their own vector databases, where each entry contains questions and answers. For example, I could input a set of questions and answers specifically related to privacy. The retrieval model will retrieve appropriate contexts from the database to refine the LLM output. This project has shown my research and problem-solving skills, and technical skills such as database management, LLM prompt engineering, and web development.
